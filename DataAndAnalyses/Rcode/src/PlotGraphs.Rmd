---
title: "Results"
author: "Jenny C. A. Read"
date: "21/01/2021"
output:
  word_document: default
  html_document: default
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE, echo=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(ggbeeswarm)
library(lme4)
library(tidyr)
library(lmerTest)
theme_set(theme_classic())

results = read.csv("../output/results.csv")

# For some plots, we will want data in wider format
tmp = results[,c("ParticipantIdentifier","AUROC","R","dprime","WordFreq","OrderCondition")]
freqresults = pivot_wider(tmp,names_from="WordFreq",values_from=c("AUROC","R","dprime"))


# Some initial processing. First of all, gamma fitting can't cope with 0s so replace these:
results$dprime[results$dprime==0] = 1e-6
results$R[results$R==0] = 1e-6
# Make a normalised AUROCnorm in the range 0-1 and ensure it is positive
results$AUROCnorm = (results$AUROC-0.5)*2
results$AUROCnorm[results$AUROCnorm<0] = 1e-6
# Our fitted AUROC should of course already be positive, but it ends up being -6.3342e-05 for some fits,
# I'm assuming due to rounding error etc.
# We will also be fitting separately for fits done using all data vs using subsets
results_AllFreq = results[results$WordFreq=="AllFreq",]
results_ByFreq = results[results$WordFreq!="AllFreq",]

```

# Data exploration with graphs
## Overall performance as measured by AUROC
We first look at overall performance as measured by the area under the receiver operating characteristic curve. Recall that our fitting procedure means that this cannot be below 0.5.
This plot shows AUROC, fitted using all words, plotted by order condition. There is a weak tendency for higher AUROC in the "even" condition.
```{r , echo=FALSE, warning=FALSE, results=FALSE}
ggplot(data=results_AllFreq,aes(x=OrderCondition,y=AUROC))+ geom_boxplot()+ geom_quasirandom(aes(color=StartCondition) )
```
Here, we plot the same data but now draw separate box plots by start condition, to see if that makes a difference. It does not seem to.
```{r , echo=FALSE, warning=FALSE, results=FALSE}
ggplot(data=results_AllFreq,aes(x=OrderCondition,y=AUROC,color=StartCondition))+ geom_boxplot()+ geom_quasirandom() 
```

This plot shows AUROC fitted using only low- or high-freq words, again plotted by order condition. In both cases, dprime is slightly higher for low-frequency words.
```{r , echo=FALSE, warning=FALSE, results=FALSE}
ggplot(data=results_ByFreq,aes(x=OrderCondition,y=AUROC,color=WordFreq))+ geom_boxplot()+ geom_quasirandom() 
```

This plot shows AUROC fitted for a given subject, using the high-freq words vs low-freq words. There is a lot of scatter but the correlations are highly significant in both cases. A Wilcoxon rank sum test finds a significant difference in AUROC between low- and high-frequency words.
```{r , echo=FALSE, warning=FALSE, results=FALSE}
 ggplot(data=freqresults,aes(x=AUROC_LoFreq,y=AUROC_HiFreq,color=OrderCondition))+ geom_point() + geom_smooth(method="lm") + geom_abline(intercept=0,slope=1,color="black")

r = cor.test(freqresults$AUROC_LoFreq,freqresults$AUROC_HiFreq,method = "spearman")
w= wilcox.test(freqresults$AUROC_LoFreq,freqresults$AUROC_HiFreq,paired=TRUE)

```



## Familiarity parameter Dprime
Now we do the same for dprime.

This plot shows dprime, fitted using all words, with separate box plots for order condition. There does not seem to be any effect either of order or of start condition.
```{r , echo=FALSE, warning=FALSE, results=FALSE}
ggplot(data=results_AllFreq,aes(x=OrderCondition,y=dprime,color=StartCondition))+ geom_boxplot()+ geom_quasirandom() 
```

This plot shows dprime fitted using only low- or high-freq words, again plotted by order condition. There does not seem to be any significant effect of word frequency on dprime.
```{r , echo=FALSE, warning=FALSE, results=FALSE}
ggplot(data=results_ByFreq,aes(x=OrderCondition,y=dprime,color=WordFreq))+ geom_boxplot()+ geom_quasirandom() 
```

This plot shows dprime fitted for a given subject, using the high-freq words vs low-freq words. There is a lot of scatter but the correlations are highly significant in both cases. A Wilcoxon rank sum test finds no significant difference in  dprime between low- and high-frequency words.
```{r , echo=FALSE, warning=FALSE, results=FALSE}
 ggplot(data=freqresults,aes(x=dprime_LoFreq,y=dprime_HiFreq,color=OrderCondition))+ geom_point() + geom_smooth(method="lm") + geom_abline(intercept=0,slope=1,color="black")

r = cor.test(freqresults$dprime_LoFreq,freqresults$dprime_HiFreq,method = "spearman")
w= wilcox.test(freqresults$dprime_LoFreq,freqresults$dprime_HiFreq,paired=TRUE)

```


## Recollection parameter R
Now we do the same for R. This plot shows R, fitted using all words, plotted by order condition. There is a weak tendency for higher R in the "even" condition.
```{r , echo=FALSE, warning=FALSE, results=FALSE}
ggplot(data=results_AllFreq,aes(x=OrderCondition,y=R))+ geom_boxplot()+ geom_quasirandom(aes(color=StartCondition) )
```
Here, we plot the same data but now draw separate box plots for start condition, to see if that makes a difference. It does not seem to.
```{r , echo=FALSE, warning=FALSE, results=FALSE}
ggplot(data=results_AllFreq,aes(x=OrderCondition,y=R,color=StartCondition))+ geom_boxplot()+ geom_quasirandom() 
```

This plot shows R fitted using only low- or high-freq words, again plotted by order condition. In both cases, R is convincingly higher for low-frequency words.
```{r , echo=FALSE, warning=FALSE, results=FALSE}
ggplot(data=results_ByFreq,aes(x=OrderCondition,y=R,color=WordFreq))+ geom_boxplot()+ geom_quasirandom() 

```


This plot shows R fitted for a given subject, using the high-freq words vs low-freq words. There is a lot of scatter but the correlation is highly significant. A Wilcoxon rank sum test finds a significant difference in R between low- and high-frequency words.

```{r , echo=FALSE, warning=FALSE, results=FALSE}
ggplot(data=freqresults,aes(x=R_LoFreq,y=R_HiFreq,color=OrderCondition))+ geom_point() + 
  geom_smooth(method="lm") + geom_abline(intercept=0,slope=1,color="black")

r = cor.test(freqresults$R_LoFreq,freqresults$R_HiFreq,method = "spearman")
w = wilcox.test(freqresults$R_LoFreq,freqresults$R_HiFreq,paired=TRUE)
print(w)
```


## Relationship between R and  d'
This shows R vs d', for fits to all data.
```{r correlation, echo=FALSE, warning=FALSE, results=FALSE}

g = ggplot(data=results_AllFreq,aes(x=dprime,y=R,color=OrderCondition))+ geom_point() + geom_smooth(method="lm")
print(g)
#tiff(filename="..\\output\\Fig_Correlation.tif")
#print(g)
#dev.off()
rp = cor.test(results$dprime,results$R,method = "pearson")
rs = cor.test(results$dprime,results$R,method = "spearman")
```

There is a significant positive correlation between the $R$ and $d'$ parameters: 
Pearson correlation: rho =`r formatC(signif(rp$estimate,digits=2))`, p=`r formatC(signif(rp$p.value,digits=2))`,
Spearman correlation: rho =`r formatC(signif(rs$estimate,digits=2))`, p=`r formatC(signif(rs$p.value,digits=2))`.

Note that in any individual fit there tends to be a tradeoff between $R$ and $d'$. So if people were identical and any differences were due to uncertainty in the fit, we would expect a negative correlation. The positive correlation indicates that people are different: those with good recollection also tend to have good $d'$. Note that the slope is the same for both groups, but the R is higher for the "even" group, as we have seen.


# Statistical analysis: Fitting with gamma distribution


## AUROC
AUROC has a peak at its lower bound of 0.5, but is not terribly skewed. Fitting it with a gamma distribution produced a singular model. I have therefore used a standard linear model when fitting AUROC.

### AUROC is significantly better for low-frequency words and for even distributions
We fitted AUROCnorm with a mixed-effect model, with two between-subjects factors (Order, even vs clumped, and Start, high vs low), and one within-subjects factor (word frequency during exposure phase, high vs low), with participant as a random factor.
"AUROC ~ WordFreq + OrderCondition  + StartCondition + (1|ParticipantIdentifier)"

This found a highly significant effect of word frequency, a significant effect of order but no effect of start condition. We also examined "AUROC ~ WordFreq * OrderCondition  + (1|ParticipantIdentifier)" but found no interaction.
The best model is therefore "AUROC ~ WordFreq + OrderCondition + (1|ParticipantIdentifier)"
```{r , echo=FALSE, warning=FALSE, results=FALSE}
# Models I examined but do not print out:
m = lmerTest::lmer(AUROCnorm ~ WordFreq + OrderCondition  + StartCondition + (1|ParticipantIdentifier), data = results_ByFreq)
 
m = lmerTest::lmer(AUROCnorm ~ WordFreq* OrderCondition  + (1|ParticipantIdentifier), data = results_ByFreq)

# Best model which I am therefore printing out:
m = lmerTest::lmer(AUROC ~ WordFreq+ OrderCondition  + (1|ParticipantIdentifier), data = results_ByFreq)
print(summary(m))
```

We also examined the effect of order and start condition on the data fitted using all words (thus containing only between-subjects factors). This again revealed a highly significant effect of order, with performance being much higher for "even" than for "clumped".

```{r , echo=FALSE, warning=FALSE, results=FALSE}
# Started from this model with main effects  & interaction
m = lm(AUROC ~ OrderCondition*StartCondition, data = results_AllFreq)
drop1(m,test="Chisq")
# used drop1 to drop terms not worth including; ended up with this:

m = lm(AUROC ~ OrderCondition, data = results_AllFreq, family = Gamma(link=log))
print(summary(m))
```



### dprime is significantly better for low-frequency words and for even distributions
We then looked at dprime and R separately to tease apart their contributions to overall performance.
Both R and dprime are constrained to be positive, so the residuals are highly non-Gaussian.
We therefore fitted the data using a gamma distribution with a logistic link function. We used glm from base stats when fitting only between-subjects factors like order, and glmer from package lmer4 to fit a mixed-model when we added the within-subject factor of word frequency.

Both these require positive data when using a gamma distribution, and for some of our subjects the fitted R, dprime were exactly zero. To enable the fitting, we therefore replaced 0 with 1e-6 in both cases. 

For dprime, I found that with a gamma distribution, fitting a mixed-effect model to the fits by frequency always produced a singular model. There was no significant effect of any factor. 

Fitting an ordinary linear model produced marginally significant effects of word frequency and order:

```{r , echo=FALSE, warning=FALSE, results=FALSE}
m = lme4::glmer(dprime ~ WordFreq + OrderCondition + StartCondition  + (1|ParticipantIdentifier),  data = results_ByFreq, family = Gamma(link=log))
# fit is singular

m = lme4::glmer(dprime ~ WordFreq + (1|ParticipantIdentifier),  data = results_ByFreq, family = Gamma(link=log))
# fit is singular

m = lmerTest::lmer(dprime ~ WordFreq + OrderCondition + StartCondition  + (1|ParticipantIdentifier),  data = results_ByFreq)
# Fit is ok, wordreq and order are significant

m = lmerTest::lmer(dprime ~ WordFreq + OrderCondition  + (1|ParticipantIdentifier),  data = results_ByFreq)
print(summary(m))
```
However, fitting a model with interactions suggested that there was a significant interaction effect. As far as I can see, what is going on is this: performance is signifcantly lower in the clumped condition (without any effect of word frequency); within the even condition, performance is significantly higher for low frequency words.
```{r , echo=FALSE, warning=FALSE, results=FALSE}
m = lmerTest::lmer(dprime ~ WordFreq * OrderCondition  + (1|ParticipantIdentifier),  data = results_ByFreq)
print(summary(m))

```

When we look at the fits to all data, the effect of order is marginally non-significant, whether we test with a gamma distribution or a Gaussian.
```{r , echo=FALSE, warning=FALSE, results=FALSE}
m = glm(dprime ~  OrderCondition   , data = results_AllFreq, family = Gamma(link=log))
m = lm(dprime ~  OrderCondition   , data = results_AllFreq)
print(summary(m))
```

I thought it would be worth looking at what the model fit implies. Here it is. The histograms show the distribution of fitted dprime for the two cases, and the lines show the fitted gamma distributions.
```{r , echo=FALSE, warning=FALSE, results=FALSE}
m = glm(dprime ~ OrderCondition, data = results_AllFreq, family = Gamma(link=log))
print(summary(m))
rr=seq(0.0001,max(results_AllFreq$dprime),length=100)
meanClumped = exp(m$coefficients[1]) # intercept term
meanEven = exp(m$coefficients[1]+m$coefficients[2]) # intercept term + x=1
#dispersion = 0.034 # this is 1/shape parameter of gamma
shape = MASS::gamma.shape(m)

fitEven = data.frame(rr)
fitEven$pdf = dgamma(rr,shape=shape$alpha,scale = meanEven*dispersion)
fitClumped = data.frame(rr)
fitClumped$pdf = dgamma(rr,shape=shape$alpha,scale = meanClumped*dispersion)
ggplot()+ 
    geom_histogram(data=results_AllFreq,aes(x=dprime,y=..density..,fill=OrderCondition)) +
    geom_line(data=fitClumped,aes(x=rr,y=pdf),color="red")+
    geom_line(data=fitEven,aes(x=rr,y=pdf),color="blue")+
  coord_cartesian(xlim=c(0,4),ylim=c(0,2))
```



# Comparing memory parameters for clumped vs even conditions
In this section, I use all data (low and high frequency words).

## Overall performance, area under receiver operating characteristic curve

```{r auroc, echo=FALSE, warning=FALSE, results=FALSE}

g = ggplot(data=results,aes(x=OrderCondition,y=AUROC))+ geom_boxplot()+ geom_quasirandom(aes(color=WordFreq)) 
print(g)
tiff(filename="..\\output\\Fig_AUROC.tif")
print(g)
dev.off()

wa = wilcox.test(AUROC~OrderCondition,data=results)
```

There is a significant difference in overall performance between the two conditions: p=`r formatC(signif(wa$p.value,digits=3))`, Wilcoxon-Mann-Whitney.


## Familiarity parameter d'

```{r familarity, echo=FALSE, warning=FALSE, results=FALSE}

g = ggplot(data=results,aes(x=OrderCondition,y=dprime))+ geom_boxplot()+ geom_quasirandom(aes(color=StartCondition)) 
print(g)
tiff(filename="..\\output\\Fig_Dprime.tif")
print(g)
dev.off()

wd = wilcox.test(dprime~OrderCondition,data=results)
```

There is no significant difference in the familiarity parameter $d'$ between the two conditions: p = `r formatC(signif(wd$p.value,digits=3))`, Wilcoxon-Mann-Whitney.



## Recollection parameter R

```{r recollection, echo=FALSE, warning=FALSE, results=FALSE}

g = ggplot(data=results,aes(x=OrderCondition,y=R))+ geom_boxplot()+ geom_quasirandom(aes(color=StartCondition)) 
print(g)
tiff(filename="..\\output\\Fig_Recollection.tif")
print(g)
dev.off()

wR = wilcox.test(R~OrderCondition,data=results)
```

There is a significant difference in the recollection parameter $R$ between the two conditions: p = `r formatC(signif(wR$p.value,digits=3))`, Wilcoxon-Mann-Whitney.

Thus, the difference in performance must be driven by the difference in $R$, not $dprime$.




### Check robustness

Tracking how significance varies as a function of the amount of data collected. Here, I'm plotting the significance reported by a Wilcoxon test comparing R in the even vs clumped condition, for the first N subjects (N plotted along x-axis). I don't know if the number of subjects was set in advance but want to check the result isn't very sensitive to exactly when we stopped.

```{r , echo=FALSE, warning=FALSE, results=FALSE}
results_AllFreq = results_AllFreq[order(results_AllFreq$CompletionDate),]

n = nrow(results_AllFreq)
p = replicate(n,NA)
for (j in 15:n) {
    wR2 = wilcox.test(R~OrderCondition,data=results_AllFreq[1:j,])
    p[j] = wR2$p.value
}
robustness = data.frame(ndata = seq(1:n), Rsignificance = p)
ggplot(data=robustness,aes(x=ndata,y=Rsignificance)) + geom_line(color="red") + geom_hline(yintercept = 0.05, color="blue")
```

A little bit concerning that had data collection stopped slightly earlier, the result would be NS? 




